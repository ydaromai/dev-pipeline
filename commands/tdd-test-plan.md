# /tdd-test-plan — PRD + UI Contract → Test Plan

You are executing the **tdd-test-plan** pipeline stage. Generate a comprehensive, tiered test plan from an approved PRD and extracted UI contract, assigning every test specification a unique traceability ID (`TP-{N}`) before any application code is written.

**Input:** PRD path via `$ARGUMENTS` (e.g., `@docs/prd/my-feature.md`), UI contract at `docs/tdd/<slug>/ui-contract.md`, and any schema files referenced in the PRD
**Output:** `docs/tdd/<slug>/test-plan.md`

---

## Step 1: Read Inputs

Read the following files:

1. The PRD file provided via `$ARGUMENTS`
2. The UI contract from `docs/tdd/<slug>/ui-contract.md` (generated by Stage 3 — Mock Analysis)
3. Any schema files referenced in the PRD (e.g., Zod schemas, TypeScript interfaces, JSON Schema definitions) — scan the PRD for file path references under data shapes, API contracts, or validation rules sections
4. `pipeline.config.yaml` — for TDD config settings (`tdd` section) and project paths

If a schema file reference cannot be resolved, log a Warning and continue. The PRD and UI contract are mandatory — if either is missing, halt with a Critical error identifying the missing file.

---

## Step 2: Generate Tiered Test Specifications

Produce two tiers of test specifications from the PRD and UI contract. Every test item across both tiers receives a unique `TP-{N}` traceability ID (sequential starting from TP-1). IDs must be globally unique within the test plan — no duplicates, no gaps.

### Tier 1 — E2E / Playwright (Full Specifications)

For each testable user-facing requirement in the PRD, combined with the UI contract's DOM structure and interactive element inventory, produce a **full test specification** containing:

- **TP-{N}:** Unique traceability ID
- **Tier:** `Tier 1`
- **AC Reference:** The PRD acceptance criterion this test validates (e.g., `AC 3.2`)
- **Test Title:** Concise, descriptive title
- **Preconditions:** Required state before test execution (e.g., logged in, specific route loaded, data seeded)
- **Test Steps:** Numbered step-by-step actions using Playwright API conventions
  1. Navigate to route
  2. Interact with element (click, fill, select)
  3. Wait for expected state change
  4. Assert outcome
- **Selectors:** Use `data-testid` values from the UI contract's Data-Testid Registry. Reference selectors as `[data-testid="<value>"]`. If a required element has no data-testid in the registry, flag it as a Warning and propose a candidate following the kebab-case convention.
- **Expected Outcome:** What the user should see or what the DOM state should be after the test steps
- **Assertions:** Specific assertion statements (e.g., `expect(element).toBeVisible()`, `expect(page).toHaveURL('/dashboard')`, `expect(errorMessage).toContainText('Required field')`)

### Tier 2 — Integration / Unit (Specification Outlines)

For each testable technical requirement, internal behavior, data transformation, or component contract identified in the PRD and schema files, produce a **specification outline** containing:

- **TP-{N}:** Unique traceability ID
- **Tier:** `Tier 2`
- **AC Reference:** The PRD acceptance criterion this test validates (e.g., `AC 4.1`)
- **Test Intent:** One-sentence description of what the test verifies
- **Test Type:** `integration` or `unit`

Tier 2 outlines are intentionally minimal. Full test code for Tier 2 is developed in Stage 7 (Develop App) alongside application code, when component boundaries and internal architecture are known.

---

## Step 3: Mandatory Contract Sections

After the tiered specifications, include four mandatory contract sections. Each contract section contains test specifications (with TP-{N} IDs) that cut across the tiered structure. A single TP-{N} may appear in a contract section AND in the tiered listing above — the contract section groups related tests for review completeness, not to create duplicates.

### Performance Contracts

Define performance-related test expectations:

- **Response time budgets:** Maximum acceptable time for route navigation, API responses, and data rendering (e.g., "Dashboard loads within 2s on 3G throttle")
- **Rendering budgets:** Maximum acceptable time for component mount, re-render after state change, and large list rendering
- **Resource budgets:** Maximum bundle size impact, memory consumption bounds if specified in PRD
- Reference PRD NFRs and any performance-related acceptance criteria
- Each contract item references its TP-{N} ID(s)

### Accessibility Contracts

Define accessibility-related test expectations:

- **WCAG 2.1 AA compliance:** Specific criteria applicable to each route/component (color contrast, text sizing, target sizing)
- **Keyboard navigation:** Tab order matches UI contract's Accessibility Map, all interactive elements reachable via keyboard, focus indicators visible
- **Screen reader:** ARIA roles and labels from UI contract match expected announcements, dynamic content updates announced via live regions
- **Focus management:** Modal focus trapping, route change focus reset, skip navigation links
- Each contract item references its TP-{N} ID(s)

### Error Contracts

Define error handling test expectations:

- **Validation errors:** Every form field's validation rules (from UI contract Form Contracts) mapped to expected error messages and display behavior
- **Network errors:** Behavior when API calls fail (loading states, error messages, retry affordances)
- **Empty states:** Behavior when data sets are empty (empty state components, call-to-action messaging)
- **Boundary errors:** Behavior at data limits (max length inputs, overflow, pagination boundaries)
- Each contract item references its TP-{N} ID(s)

### Data Flow Contracts

Define data integrity test expectations:

- **Data shapes:** Expected input/output shapes for each data transformation, referencing schema files where available
- **Validation rules:** Field-level validation (type, format, range, required) matching schema definitions
- **Transformation correctness:** Data mapping between API responses and UI display (formatting, sorting, filtering, aggregation)
- **State persistence:** Data that must survive route navigation, form multi-step flows, or page refresh
- Each contract item references its TP-{N} ID(s)

---

## Step 4: Critic Review

Run the 10-critic Ralph Loop on the test plan document.

Spawn all applicable critic subagents in parallel using the Task tool:

**Product Critic (model: opus — Opus 4.6):**
```
You are the Product Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/product-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review whether the test plan fully covers the PRD:
- Does every P0 acceptance criterion have at least one TP-{N} test specification?
- Are there PRD requirements with no corresponding test?
- Do Tier 1 E2E specs cover all user-facing flows?
- Are Tier 2 outlines sufficient for Stage 7 implementation?

Produce your structured output.
```

**Dev Critic (model: opus — Opus 4.6):**
```
You are the Dev Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/dev-critic.md (your persona)
2. The test plan: <paste test plan content>

Review the test plan for:
- Are Tier 1 test steps technically feasible with Playwright?
- Are selectors valid and referencing the data-testid registry correctly?
- Are assertions specific enough to catch real failures (not trivially passing)?
- Do Tier 2 outlines describe testable behavior (not implementation details)?

Produce your structured output.
```

**DevOps Critic (model: opus — Opus 4.6):**
```
You are the DevOps Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/devops-critic.md (your persona)
2. The test plan: <paste test plan content>

Review the test plan for:
- Are there CI/CD implications for the test tiers (parallelization, timeouts)?
- Do Performance Contracts include measurable thresholds suitable for CI gates?
- Are test environment assumptions documented?

Produce your structured output.
```

**QA Critic (model: opus — Opus 4.6):**
```
You are the QA Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/qa-critic.md (your persona)
2. The PRD (Section 9 — Testing Strategy): <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Are all acceptance criteria covered by at least one TP-{N}?
- Are Tier 1 specs detailed enough for a developer to implement without ambiguity?
- Do contract sections cover edge cases and negative paths?
- Are there redundant tests that could be consolidated?
- Do Tier 2 outlines include sufficient context for Stage 7?

Produce your structured output.
```

**Security Critic (model: opus — Opus 4.6):**
```
You are the Security Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/security-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Are there missing security test specifications (auth flows, input sanitization, CSRF, XSS)?
- Do Error Contracts cover security-sensitive error handling (no stack traces, no sensitive data leakage)?
- Are authorization boundary tests included for protected routes/actions?

Produce your structured output.
```

**Performance Critic (model: opus — Opus 4.6):**
```
You are the Performance Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/performance-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Are Performance Contracts measurable and enforceable in tests?
- Do response time budgets align with PRD NFRs?
- Are there missing performance test cases for data-heavy routes?
- Are rendering budgets realistic for the target platforms?

Produce your structured output.
```

**Data Integrity Critic (model: opus — Opus 4.6):**
```
You are the Data Integrity Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/data-integrity-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Do Data Flow Contracts cover all schema-defined data shapes?
- Are validation rules consistent between schema files and test specifications?
- Are data transformation tests present for all API-to-UI mappings?
- Do tests account for null/undefined/empty edge cases?

Produce your structured output.
```

**Observability Critic (model: opus — Opus 4.6) — Only spawn if pipeline.config.yaml has `has_backend_service: true`:**
```
You are the Observability Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/observability-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Are there test specifications for logging/metrics emission?
- Do Error Contracts verify that errors produce observable signals?

Produce your structured output.
```

**API Contract Critic (model: opus — Opus 4.6) — Only spawn if pipeline.config.yaml has `has_api: true`:**
```
You are the API Contract Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/api-contract-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Do Data Flow Contracts cover API request/response shapes?
- Are there missing contract tests for API endpoints referenced in the PRD?
- Do Tier 2 outlines include API integration tests?

Produce your structured output.
```

**Designer Critic (model: opus — Opus 4.6) — Only spawn if pipeline.config.yaml has `has_frontend: true`:**
```
You are the Designer Critic. Read:
1. ${CLAUDE_PLUGIN_ROOT}/pipeline/agents/designer-critic.md (your persona)
2. The PRD: <paste PRD content>
3. The test plan: <paste test plan content>

Review the test plan for:
- Do Accessibility Contracts cover WCAG 2.1 AA requirements for all UI components?
- Are responsive behavior tests included for all three viewports (375, 768, 1280)?
- Do Error Contracts include user-facing error state visual expectations?
- Are loading states and empty states covered?

Produce your structured output.
```

### Revision Loop

**Pass condition:** ALL critics must report 0 Critical findings AND 0 Warnings. Notes (informational) are acceptable.

If any critic has Critical findings OR Warnings:
1. Read all Critical findings and Warnings from all critics
2. Revise the test plan to address ALL findings (Critical first, then Warnings)
3. Re-run ALL critics (max 5 total iterations)
4. If still not clean after max iterations:
   - Present remaining findings to user
   - Options: continue iterating | approve with remaining warnings | edit manually | abort

---

## Step 5: Write Output

Create the output directory if needed:
```bash
mkdir -p docs/tdd/<slug>
```

Write the test plan to `docs/tdd/<slug>/test-plan.md`.

The test plan document must include this header:

```markdown
# Test Plan: <feature title>

**PRD:** `<prd_path>`
**UI Contract:** `docs/tdd/<slug>/ui-contract.md`
**Generated:** <date>
**Status:** APPROVED (Ralph Loop iteration N, 0C/0W)

---

## Summary

- **Total TP count:** N
- **Tier 1 (E2E/Playwright):** N specifications
- **Tier 2 (Integration/Unit):** N outlines
- **Contract sections:** Performance, Accessibility, Error, Data Flow

---
```

Followed by the Tier 1 specifications, Tier 2 outlines, and the four mandatory contract sections.

---

## Step 6: Human Gate

Present a summary to the user:

```
Test plan generated: docs/tdd/<slug>/test-plan.md

## Test Plan Summary

### TP Count by Tier
- Tier 1 (E2E/Playwright): N full specifications
- Tier 2 (Integration/Unit): N outlines
- Total: N test specifications

### Contract Coverage
- Performance Contracts: N items (TP-X, TP-Y, ...)
- Accessibility Contracts: N items (TP-X, TP-Y, ...)
- Error Contracts: N items (TP-X, TP-Y, ...)
- Data Flow Contracts: N items (TP-X, TP-Y, ...)

### Traceability Overview
- PRD acceptance criteria covered: N / M (percentage)
- ACs without test coverage: [list any gaps]

### Critic Results
- Product Critic: PASS (0 Critical, 0 Warnings)
- Dev Critic: PASS (0 Critical, 0 Warnings)
- DevOps Critic: PASS (0 Critical, 0 Warnings)
- QA Critic: PASS (0 Critical, 0 Warnings)
- Security Critic: PASS (0 Critical, 0 Warnings)
- Performance Critic: PASS (0 Critical, 0 Warnings)
- Data Integrity Critic: PASS (0 Critical, 0 Warnings)
- Observability Critic: PASS / N/A (0 Critical, 0 Warnings)
- API Contract Critic: PASS / N/A (0 Critical, 0 Warnings)
- Designer Critic: PASS / N/A (0 Critical, 0 Warnings)
Ralph Loop iterations: N

Please review the test plan. You can:
1. Approve it as-is
2. Request changes (add/remove/modify test specifications)
3. Edit the file directly and re-run validation
4. Abort
```

Wait for user approval before proceeding to Stage 5 (Dev Plan).
